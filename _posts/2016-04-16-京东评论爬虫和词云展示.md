## 写在前面

这本是数据挖掘课上老师布置的作业，做完后便记录在这里。作业的要求是：利用爬虫抓取文本数据，进行文本分词，对分词后的数据进行高频词排名，并绘制词云。  

正巧自己最近在啃周志华老师的《机器学习》，所以，就决定爬京东上这本书的评论来做这个作业。   

## 爬虫

爬虫和清理数据用的的R中的RCurl包和XML包

``` R
library(RCurl)
library(XML)
```



京东的评论网址一般是这样的：http://club.jd.com/review/11867803-1-1-0.html在抓取的时候，遇到的问题是：循环中每抓20页之后就报错，猜测应该是京东设置了什么反爬虫的机制吧。我的解决办法是暴力解决，不让抓就不停地抓，一直抓到为止，用了R中的try函数。

``` R
url <- paste0('http://club.jd.com/review/11867803-1-',1:26,'-0.html')
comment_all <- c(); date_all <- c(); nurl_all <- c()
for (i in 1:length(url)){
  htmldoc <- try(htmlParse(url[i],encoding='GBK'))
  while(inherits(htmldoc, 'try-error')){
    Sys.sleep(5)
    htmldoc <- try(htmlParse(url[i],encoding='GBK'))
  }
  rootdoc <- xmlRoot(htmldoc)
  comment <- xpathSApply(rootdoc, 
                   '//div[@class="item"]/div[@class="i-item"]/div[@class="comment-content"]', xmlValue)
  date <- xpathSApply(rootdoc,
                   '//div[@class="item"]/div[@class="i-item"]/div[@class="o-topic"]/span[@class="date-comment"]',xmlValue)
  nurl <- xpathSApply(rootdoc,
                   '//div[@class="item"]/div[@class="i-item"]/div[@class="btns"]/a[@class="btn-reply"]', xmlGetAttr,'href')
  
  comment_all <- c(comment_all,comment)
  date_all <- c(date_all,date)
  nurl_all <- c(nurl_all,nurl)
}
```







